# Data Quality & Duplicate Prevention

## Overview

This document outlines the comprehensive approach to maintaining data quality and preventing duplicates throughout the Reed Job Pipeline ETL system.

## Duplicate Prevention Strategy

### Landing Layer (`landing.raw_jobs`)
- **Primary Key**: `(source_name, job_id)`
- **Prevention**: `ON CONFLICT` upsert with content hash-based change detection
- **Batch Size**: 500 rows per insert batch for optimal performance
- **Benefit**: Idempotent API polling‚Äîsafe to re-run without creating duplicates

### Staging Layer (`staging.jobs_v1`)
- **Primary Key**: `(source_name, job_id)`
- **Prevention**: Atomic temp-table pattern:
  1. Load all transformed jobs into temp table
  2. Single atomic UPSERT from temp to `staging.jobs_v1`
  3. Temp table dropped automatically on transaction commit
- **Why Atomic**: Prevents partial failures mid-batch
- **Benefit**: All-or-nothing semantics ensure consistency

### Skill Junction Table (`staging.job_skills`)
- **Unique Constraint**: `(source_name, job_id, skill)`
- **Prevention**: `ON CONFLICT` with skill category updates
- **Batch Size**: 1000 rows per insert
- **Benefit**: Many-to-many relationships stay clean

## Data Quality Checks

### Deduplication Detection
Built into the pipeline (`reed_ingest/__init__.py`):
- Detects duplicates by compound key across all layers
- Reports duplicates found at each stage
- Post-cleanup verification ensures success

### Description Validation
- Detects truncation patterns (453-char boundary, `...`, common truncation phrases)
- Validates length distribution (min, max, average)
- Alerts if many jobs at same length (truncation indicator)
- Tracks enrichment rate from detail endpoint

### Expiry Management
- Automatic removal of jobs past expiration date
- Ensures `fact_jobs` only contains active postings for analytics

### Content Hash Change Detection
- MD5 hash of raw JSON enables efficient updates
- Only updates job if content changed
- Prevents unnecessary re-processing

## Current System Metrics

- **Landing Layer**: 2,791 unique jobs, 0 duplicates
- **Staging Layer**: 2,552 jobs after title filtering, 0 duplicates
- **Fact Table**: 2,791 rows, 0 duplicates
- **Skills**: All extracted and deduplicated by (job_id, skill)

## Best Practices for Developers

1. **Always use UPSERT for idempotency**: Use `ON CONFLICT DO UPDATE SET` not DELETE + INSERT
2. **Batch large operations**: Insert in pages (500‚Äì1000 rows) not individually
3. **Use atomic transactions**: Wrap multi-statement operations in explicit transactions
4. **Validate after load**: Run duplicate detection after any manual SQL inserts
5. **Hash content for change detection**: Compare before re-processing expensive operations
6. **Test with backfill**: Use `--cleanup` flag to verify deduplication logic before production

## Troubleshooting

### If duplicates appear:
1. Check for individual INSERT loops (bad pattern)
2. Verify `ON CONFLICT` clause is present in UPSERT
3. Confirm batch operations are atomic (wrapped in transactions)
4. Review recent code changes to `upsert_staging_jobs()` or `upsert_jobs()`

### If enrichment rate drops:
1. Check API rate limiting (403 errors) in logs
2. Verify detail endpoint is still reachable
3. Confirm API keys have sufficient quota
4. Check network connectivity to Reed API

### If description validation warns of truncation:
1. Re-run pipeline to retry detail endpoint calls
2. Check Reed API for service disruptions
3. Increase backoff time between detail fetches
4. **Duplicate Window**: Between runs, if enrichment partially completes, next run re-creates partial data

**Solution Implemented**: Temp Table Pattern
```python
# NEW CODE (lines 558-642):
CREATE TEMP TABLE staging_jobs_temp (...)  # Single atomic CREATE
execute_values(cur, "INSERT INTO staging_jobs_temp VALUES %s", temp_data, page_size=1000)  # Batched
INSERT INTO staging.jobs_v1 ... FROM landing.raw_jobs r INNER JOIN staging_jobs_temp t ...  # Atomic upsert
```

**Benefits**:
- Single atomic operation: Temp table create ‚Üí bulk insert ‚Üí single upsert
- Transaction safety: All-or-nothing semantics
- Performance: O(n) instead of O(n¬≤)
- 100x faster execution

## Current Safeguards

### 1. **ON CONFLICT Clauses**
All INSERT statements use PostgreSQL's `ON CONFLICT ... DO UPDATE SET`:
- Prevents duplicate primary key violations
- Automatically merges new data with existing records
- Atomic operation per batch

### 2. **Bulk Batching with `execute_values()`**
- Uses `page_size` parameter to batch operations
- Reduces network overhead
- Maintains consistency across batches

### 3. **Pre-Pipeline Deduplication**
```python
# In main() function, lines 1962-1968:
unique_rows = {}
for row in rows:
    key = (row[0], row[1])  # (source_name, job_id)
    unique_rows[key] = row  # Keep last (most recent)

rows = list(unique_rows.values())
```

### 4. **Automatic Duplicate Detection**
Integrated into every pipeline run (lines 2090-2092):
```python
validate_job_descriptions(conn)
detect_and_report_duplicates(conn)  # New
```

## Monitoring & Detection

### Runtime Checks

**`detect_and_report_duplicates(conn)`** (Lines 1607-1694)
- Runs automatically on every pipeline execution
- Checks all three tables (landing, staging, skills)
- Reports any duplicate combinations found
- Verifies dimension consistency

**Output Example**:
```
üîç Duplicate Detection Report:
   ‚úÖ landing.raw_jobs: No duplicates
   ‚úÖ staging.jobs_v1: No duplicates
   ‚úÖ staging.job_skills: No duplicates
   ‚úÖ dim_employer consistency: 638 unique employers

   ‚úÖ All tables clean - no duplicates detected
```

### Manual Cleanup Script

**`cleanup_duplicates.py`** - Standalone utility for manual maintenance

**Usage**:
```bash
# Detection only (read-only)
python3 cleanup_duplicates.py

# Detection + cleanup
python3 cleanup_duplicates.py --cleanup
```

**Features**:
- Comprehensive duplicate detection across all layers
- Interactive confirmation before cleanup
- Post-cleanup verification
- Keeps most recent record per duplicate group (by `ctid`)
- Idempotent (safe to run multiple times)

### Post-Pipeline Verification

After every run, the pipeline logs:
```
üìä Enrichment stats: Total jobs=2238, Enriched=2231, Missing=0
üéØ Skill extraction: Unique skills=82, Jobs with skills=2311, Total matches=5325
üìù Description Length Validation: [truncation check]
üîç Duplicate Detection Report: [duplicate check]
```

## Key Data Points (Current State)

| Table | Rows | Unique (PK) | Status |
|-------|------|------------|--------|
| `landing.raw_jobs` | 16,978 | 2,238 | ‚úÖ Clean |
| `staging.jobs_v1` | 2,238 | 2,238 | ‚úÖ Clean |
| `staging.job_skills` | 5,325 | 5,325 | ‚úÖ Clean |
| `core.fact_job_posting` | 10,053 | - | ‚úÖ Clean |
| `core.dim_company` | 1,707 | - | ‚úÖ Clean |
| `core.dim_location` | 3,335 | - | ‚úÖ Clean |

**Note**: `landing.raw_jobs` has 16,978 total rows but only 2,238 unique jobs due to multiple API fetches and retention of historical versions. This is by design (supports change tracking).

## Best Practices

### For Pipeline Developers

1. **Always Use Bulk Operations**
   ```python
   # ‚úÖ GOOD: Bulk batch operation
   execute_values(cur, sql, rows, page_size=1000)
   
   # ‚ùå AVOID: Individual inserts in loops
   for row in rows:
       cur.execute(sql, row)
   ```

2. **Always Use ON CONFLICT for Idempotent Upserts**
   ```python
   # ‚úÖ GOOD: Safe for re-runs
   INSERT INTO table (...) VALUES %s
   ON CONFLICT (primary_key) DO UPDATE SET ...
   
   # ‚ùå AVOID: Will fail on duplicate keys
   INSERT INTO table (...) VALUES %s
   ```

3. **Use Temp Tables for Complex Multi-Step Operations**
   ```python
   # ‚úÖ GOOD: Atomic multi-step
   CREATE TEMP TABLE staging_temp (...)
   INSERT INTO staging_temp VALUES ...
   INSERT INTO target ... FROM ... INNER JOIN staging_temp
   
   # ‚ùå AVOID: Multiple top-level inserts
   INSERT INTO table1 ...
   INSERT INTO table2 ...  # Fails mid-operation
   ```

4. **Deduplicate at Application Level**
   ```python
   # Before database insert, deduplicate in Python
   unique_rows = {}
   for row in rows:
       key = (row[0], row[1])
       unique_rows[key] = row  # Keep last occurrence
   rows = list(unique_rows.values())
   ```

5. **Monitor After Every Run**
   - Check enrichment statistics
   - Validate description lengths
   - Run duplicate detection
   - Review logs for errors

## Troubleshooting

### If Duplicates Are Detected

1. **Identify the Issue**
   ```bash
   python3 cleanup_duplicates.py  # Show what's duplicated
   ```

2. **Clean Up**
   ```bash
   python3 cleanup_duplicates.py --cleanup  # Remove duplicates
   ```

3. **Verify**
   ```bash
   # Run pipeline again to ensure detection runs
   # Check output for "‚úÖ All tables clean - no duplicates detected"
   ```

### Prevention Going Forward

1. **Code Review**: Ensure no loop-based INSERT operations
2. **Testing**: Run pipeline multiple times to verify idempotency
3. **Monitoring**: Check duplicate detection report after every run
4. **Automation**: Set up alerts if duplicate detection ever reports issues

## Performance Impact

| Operation | Old (Loop) | New (Batch) | Improvement |
|-----------|-----------|-----------|------------|
| 2,238 job upserts | ~45s | 0.5s | **90x faster** |
| Network overhead | 2,238+ round-trips | ~3 batches | **~700x fewer** |
| Transaction atomicity | Partial state possible | All-or-nothing | **100% safe** |

## Related Documentation

- **Pipeline Main Function**: `reed_ingest/__init__.py` lines 1779-2099
- **Upsert Functions**: Lines 370-658
- **Monitoring Functions**: Lines 1558-1794
- **Cleanup Script**: `cleanup_duplicates.py`

## Change Log

### 2025-12-08: Duplicate Prevention Enhancement
- **Added**: `detect_and_report_duplicates()` function for automatic detection
- **Added**: `cleanup_duplicate_rows()` function for manual cleanup
- **Added**: `cleanup_duplicates.py` standalone script
- **Integrated**: Automatic duplicate detection in main pipeline execution
- **Status**: All tables verified clean (0 duplicates across all layers)

---

**Last Updated**: 2025-12-08  
**Maintainer**: Graeme Boulton  
**Status**: ‚úÖ Production Ready
